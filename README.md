# ğŸŒŒ NASA APOD ETL Pipeline with Apache Airflow(End to End)

This project is a production-ready ETL pipeline built using **Apache Airflow**. It extracts data from NASA's Astronomy Picture of the Day (APOD) API and stores it in a **PostgreSQL** database for further analysis or exploration.

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ dags/                        # DAG definition for the ETL
â”œâ”€â”€ include/                    # Extra scripts (if any)
â”œâ”€â”€ plugins/                    # Custom Airflow plugins
â”œâ”€â”€ tests/                      # DAG integrity tests
â”œâ”€â”€ Dockerfile                  # Custom Astro runtime image
â”œâ”€â”€ docker-compose.yml          # Local Airflow setup
â”œâ”€â”€ airflow_settings.yaml       # Connections and variables config
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ packages.txt                # OS-level packages
â””â”€â”€ README.md
```

## ğŸš€ Features

- â° **Daily scheduled DAG**
- ğŸ”— Connects to **NASA's APOD API** (via HTTP connection)
- ğŸ§± Transforms and validates the JSON response
- ğŸ›¢ï¸ Loads the data into **PostgreSQL** using `PostgresHook`
- âš™ï¸ Uses Airflow best practices: decorators, catchup=False, task chaining

## ğŸ”Œ Airflow Connections Setup

1. **Postgres Connection**
   - **Conn ID**: `my_postgres_connection`
   - **Type**: `Postgres`
   - **Host**: `postgres`
   - **Schema**: `postgres`
   - **Login**: `postgres`
   - **Password**: `postgres`
   - **Port**: `5432`

2. **NASA API HTTP Connection**
   - **Conn ID**: `nasa_api`
   - **Type**: `Generic` (or use `HTTP` if available)
   - **Extra**:
     ```json
     {
       "api_key": "YOUR_NASA_API_KEY"
     }
     ```

## ğŸ“¥ DAG Details

- **DAG ID**: `nasa_apod_postgres`
- **Schedule**: `@daily`
- **Tasks**:
  - `create_table()`: Initializes `apod_data` table
  - `extract_apod()`: Fetches JSON from NASA API
  - `transform_apod_data()`: Cleans and restructures the data
  - `load_data_to_postgres()`: Inserts into PostgreSQL with conflict handling

## ğŸ“¦ Setup & Deployment

### Run Locally

```bash
# Start Airflow
astro dev start

# View UI at http://localhost:8080
# DAGs folder is auto-synced
```

### Deploy to Astronomer

```bash
astro deployment list
astro deploy
```

## ğŸ§ª Testing

```bash
astro dev run pytest
```

## ğŸ§¾ Example Query

```sql
SELECT * FROM apod_data ORDER BY date DESC LIMIT 5;
```

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

> Built with â¤ï¸ by Aman Chaurasia
